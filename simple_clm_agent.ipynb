{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CLM Dataset Search Agent\n",
    "\n",
    "This notebook demonstrates a basic agent that can:\n",
    "1. Search for datasets based on topics\n",
    "2. Answer questions about dataset metadata\n",
    "3. Maintain conversation history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip -q install python-dotenv ipywidgets pydantic-ai fastmcp openai nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import nest_asyncio\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from fastmcp import Client\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable nested asyncio for Jupyter notebooks\n",
    "nest_asyncio.apply()\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set Up API Keys\n",
    "\n",
    "You need an OpenAI API key or a NRP API key to use this agent. Create a `.env` file with:\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_key\n",
    "\n",
    "NRP_API_KEY=your_nrp_key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check API keys\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "nrp_key = os.getenv('NRP_API_KEY')\n",
    "\n",
    "if not openai_key:\n",
    "    print(\"‚ö†Ô∏è Warning: OPENAI_API_KEY not set!\")\n",
    "else:\n",
    "    print(\"‚úì OpenAI API key found\")\n",
    "\n",
    "if not nrp_key:\n",
    "    print(\"‚ö†Ô∏è Warning: NRP_API_KEY not set!\")\n",
    "else:\n",
    "    print(\"‚úì NRP API key found\")\n",
    "\n",
    "# Choose your model: \"openai\" or \"nrp\"\n",
    "MODEL = \"openai\"  # Change this to \"nrp\" to use Qwen3\n",
    "\n",
    "if MODEL == \"openai\":\n",
    "    print(\"‚úì Using OpenAI GPT-4o-mini\")\n",
    "elif MODEL == \"nrp\":\n",
    "    print(\"‚úì Using NRP Qwen3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize MCP Client\n",
    "\n",
    "The MCP (Model Context Protocol) client connects to the dataset search service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MCP client for dataset search\n",
    "mcp_client = Client(\"https://wenokn.fastmcp.app/mcp\")\n",
    "print(\"‚úì MCP client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create the Agent\n",
    "\n",
    "This is the core of our application. The agent:\n",
    "- Understands natural language questions\n",
    "- Uses the search_datasets tool to find relevant datasets\n",
    "- Maintains conversation history\n",
    "- Can answer follow-up questions about dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"Stores information about the current dataset being discussed\"\"\"\n",
    "    current_dataset: Optional[dict] = None\n",
    "\n",
    "def get_model_config(model_name: str = \"openai\"):\n",
    "    \"\"\"Get model configuration based on model name.\"\"\"\n",
    "    if model_name == \"nrp\":\n",
    "        # Configure for NRP\n",
    "        os.environ['OPENAI_BASE_URL'] = 'https://ellm.nrp-nautilus.io/v1'\n",
    "        os.environ['OPENAI_API_KEY'] = os.getenv('NRP_API_KEY', '')\n",
    "        return 'openai:qwen3'\n",
    "    else:\n",
    "        # Configure for OpenAI\n",
    "        if 'OPENAI_BASE_URL' in os.environ:\n",
    "            del os.environ['OPENAI_BASE_URL']\n",
    "        return 'openai:gpt-4o-mini'\n",
    "\n",
    "# Create the agent with selected model\n",
    "agent = Agent(\n",
    "    model=get_model_config(MODEL),\n",
    "    deps_type=AgentContext,\n",
    "    system_prompt=\"\"\"You are a helpful assistant that helps users find and learn about California Landscape Metrics datasets.\n",
    "\n",
    "You have access to a search_datasets tool that can find relevant datasets based on user queries.\n",
    "\n",
    "When a user asks about a topic:\n",
    "1. Use the search_datasets tool to find the most relevant dataset\n",
    "2. Present the top result with key information (title, description, units)\n",
    "3. Answer any follow-up questions about the dataset metadata\n",
    "\n",
    "The conversation history is provided in this format:\n",
    "User: <question1>\n",
    "Assistant: <answer1>\n",
    "User: <question2>\n",
    "\n",
    "Use this history to understand context for follow-up questions.\n",
    "\n",
    "Be concise and helpful!\"\"\"\n",
    ")\n",
    "\n",
    "@agent.tool\n",
    "async def search_datasets(\n",
    "    ctx: RunContext[AgentContext],\n",
    "    query: str,\n",
    "    top_k: int = 3\n",
    ") -> dict:\n",
    "    \"\"\"Search for datasets related to the query.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query (e.g., 'carbon turnover', 'burn probability')\n",
    "        top_k: Number of results to return (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with search results and metadata\n",
    "    \"\"\"\n",
    "    async with mcp_client:\n",
    "        result = await mcp_client.call_tool(\n",
    "            \"search_datasets\",\n",
    "            {\"query\": query, \"top_k\": top_k}\n",
    "        )\n",
    "        \n",
    "        data = result.data\n",
    "        if data.get('success') and data.get('datasets'):\n",
    "            # Store the top dataset in context for follow-up questions\n",
    "            best_dataset = data['datasets'][0]\n",
    "            ctx.deps.current_dataset = best_dataset\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'top_dataset': best_dataset,\n",
    "                'alternatives': data['datasets'][1:] if len(data['datasets']) > 1 else [],\n",
    "                'message': f\"Found: {best_dataset['title']}\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': 'No datasets found',\n",
    "                'error': data.get('error', 'Unknown error')\n",
    "            }\n",
    "\n",
    "print(f\"‚úì Agent created successfully with {MODEL}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Add Conversation History Management\n",
    "\n",
    "This wrapper class manages the conversation history, making the agent \"remember\" previous exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalAgent:\n",
    "    \"\"\"Wrapper that adds conversation history to the agent\"\"\"\n",
    "    \n",
    "    def __init__(self, agent, model_name=\"openai\"):\n",
    "        self.agent = agent\n",
    "        self.model_name = model_name\n",
    "        self.history = []  # Stores conversation history\n",
    "        # Set default timeout based on model\n",
    "        self.default_timeout = 180 if model_name == \"nrp\" else 60\n",
    "    \n",
    "    async def ask(self, question: str, timeout: int = None) -> str:\n",
    "        \"\"\"Ask a question and get a response.\n",
    "        \n",
    "        Args:\n",
    "            question: The user's question\n",
    "            timeout: Maximum seconds to wait for response (uses default if None)\n",
    "        \n",
    "        Returns:\n",
    "            The agent's response as a string\n",
    "        \"\"\"\n",
    "        # Use default timeout if not specified\n",
    "        if timeout is None:\n",
    "            timeout = self.default_timeout\n",
    "            \n",
    "        # Build the full input with history\n",
    "        if self.history:\n",
    "            full_input = \"\\n\".join(self.history) + f\"\\nUser: {question}\"\n",
    "        else:\n",
    "            full_input = f\"User: {question}\"\n",
    "        \n",
    "        try:\n",
    "            # Run the agent with timeout\n",
    "            result = await asyncio.wait_for(\n",
    "                self.agent.run(full_input, deps=AgentContext()),\n",
    "                timeout=timeout\n",
    "            )\n",
    "            \n",
    "            # Extract the response\n",
    "            response = result.output if hasattr(result, 'output') else str(result)\n",
    "            \n",
    "            # Update history\n",
    "            self.history.append(f\"User: {question}\")\n",
    "            self.history.append(f\"Assistant: {response}\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except asyncio.TimeoutError:\n",
    "            return f\"Error: Request timed out after {timeout} seconds. Try a simpler question or switch to OpenAI model.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {type(e).__name__}: {str(e)}\"\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history\"\"\"\n",
    "        self.history = []\n",
    "\n",
    "# Create the conversational agent with model-aware timeout\n",
    "conv_agent = ConversationalAgent(agent, model_name=MODEL)\n",
    "print(f\"‚úì Conversational agent ready with {MODEL} (timeout: {conv_agent.default_timeout}s)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Chat Interface\n",
    "\n",
    "A simple, user-friendly chat interface using Jupyter widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleChatInterface:\n",
    "    \"\"\"Simple chat interface for the agent\"\"\"\n",
    "    \n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.messages = []\n",
    "        \n",
    "        # Create UI components\n",
    "        self.output_area = widgets.VBox(\n",
    "            layout=widgets.Layout(\n",
    "                border='1px solid #ddd',\n",
    "                height='400px',\n",
    "                overflow_y='auto',\n",
    "                padding='10px',\n",
    "                margin='10px 0'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.input_box = widgets.Textarea(\n",
    "            placeholder='Ask about datasets (e.g., \"Find datasets about carbon turnover\")...',\n",
    "            layout=widgets.Layout(width='100%', height='80px')\n",
    "        )\n",
    "        \n",
    "        self.send_button = widgets.Button(\n",
    "            description='Send',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='100px')\n",
    "        )\n",
    "        \n",
    "        self.clear_button = widgets.Button(\n",
    "            description='Clear',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='100px', margin='0 0 0 10px')\n",
    "        )\n",
    "        \n",
    "        self.status_label = widgets.HTML(value=\"‚úÖ Ready\")\n",
    "        \n",
    "        # Connect buttons\n",
    "        self.send_button.on_click(self.on_send)\n",
    "        self.clear_button.on_click(self.on_clear)\n",
    "        \n",
    "        # Layout\n",
    "        button_row = widgets.HBox([self.send_button, self.clear_button, self.status_label])\n",
    "        self.interface = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>ü§ñ Dataset Search Agent</h3>\"),\n",
    "            self.output_area,\n",
    "            self.input_box,\n",
    "            button_row\n",
    "        ])\n",
    "        \n",
    "        # Welcome message\n",
    "        self.add_message(\n",
    "            \"Welcome! I can help you find California Landscape Metrics datasets.\\n\\n\"\n",
    "            \"Try asking:\\n\"\n",
    "            \"‚Ä¢ Find datasets about carbon turnover\\n\"\n",
    "            \"‚Ä¢ What datasets are available for burn probability?\\n\"\n",
    "            \"‚Ä¢ Tell me about the units used in this dataset\\n\"\n",
    "            \"‚Ä¢ What's the description of this dataset?\",\n",
    "            \"system\"\n",
    "        )\n",
    "    \n",
    "    def add_message(self, text, role=\"user\"):\n",
    "        \"\"\"Add a message to the chat display\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        \n",
    "        if role == \"user\":\n",
    "            color = \"#007bff\"\n",
    "            icon = \"üë§\"\n",
    "            label = \"You\"\n",
    "            bg = \"#e7f3ff\"\n",
    "        elif role == \"assistant\":\n",
    "            color = \"#28a745\"\n",
    "            icon = \"ü§ñ\"\n",
    "            label = \"Agent\"\n",
    "            bg = \"#e8f5e9\"\n",
    "        else:\n",
    "            color = \"#6c757d\"\n",
    "            icon = \"‚ÑπÔ∏è\"\n",
    "            label = \"System\"\n",
    "            bg = \"#f8f9fa\"\n",
    "        \n",
    "        message = widgets.HTML(\n",
    "            value=f\"\"\"\n",
    "            <div style='margin: 10px 0; padding: 10px; background: {bg}; \n",
    "                        border-radius: 8px; border-left: 4px solid {color};'>\n",
    "                <div style='display: flex; justify-content: space-between; margin-bottom: 5px;'>\n",
    "                    <strong style='color: {color};'>{icon} {label}</strong>\n",
    "                    <span style='color: #999; font-size: 0.85em;'>{timestamp}</span>\n",
    "                </div>\n",
    "                <div style='white-space: pre-wrap;'>{text}</div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        self.messages.append(message)\n",
    "        self.output_area.children = tuple(self.messages)\n",
    "    \n",
    "    def on_send(self, button):\n",
    "        \"\"\"Handle send button click\"\"\"\n",
    "        question = self.input_box.value.strip()\n",
    "        if not question:\n",
    "            return\n",
    "        \n",
    "        # Show user message\n",
    "        self.add_message(question, \"user\")\n",
    "        self.input_box.value = \"\"\n",
    "        \n",
    "        # Disable input while processing\n",
    "        self.send_button.disabled = True\n",
    "        self.input_box.disabled = True\n",
    "        self.status_label.value = \"<span style='color: orange;'>‚è≥ Thinking...</span>\"\n",
    "        \n",
    "        try:\n",
    "            # Get response from agent\n",
    "            response = asyncio.get_event_loop().run_until_complete(\n",
    "                self.agent.ask(question)\n",
    "            )\n",
    "            \n",
    "            # Show agent response\n",
    "            self.add_message(response, \"assistant\")\n",
    "            self.status_label.value = \"<span style='color: green;'>‚úÖ Ready</span>\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error: {str(e)}\"\n",
    "            self.add_message(error_msg, \"system\")\n",
    "            self.status_label.value = \"<span style='color: red;'>‚ùå Error</span>\"\n",
    "        \n",
    "        finally:\n",
    "            # Re-enable input\n",
    "            self.send_button.disabled = False\n",
    "            self.input_box.disabled = False\n",
    "    \n",
    "    def on_clear(self, button):\n",
    "        \"\"\"Clear the chat\"\"\"\n",
    "        self.messages = []\n",
    "        self.agent.clear_history()\n",
    "        self.output_area.children = tuple(self.messages)\n",
    "        self.add_message(\n",
    "            \"Chat cleared! Ready for new questions.\",\n",
    "            \"system\"\n",
    "        )\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the chat interface\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        display(self.interface)\n",
    "\n",
    "print(\"‚úì Chat interface ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Launch the Chat Interface\n",
    "\n",
    "Run this cell to start chatting with your agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the chat interface\n",
    "chat = SimpleChatInterface(conv_agent)\n",
    "chat.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Questions to Try\n",
    "\n",
    "1. **Find datasets about carbon turnover**\n",
    "2. **What datasets are available for burn probability?**\n",
    "3. **What are the units for this dataset?**\n",
    "4. **Can you describe this dataset in more detail?**\n",
    "5. **Search for datasets related to fire risk**\n",
    "\n",
    "## How This Works\n",
    "\n",
    "### Agent Architecture\n",
    "\n",
    "```\n",
    "User Question\n",
    "     ‚Üì\n",
    "ConversationalAgent (adds history)\n",
    "     ‚Üì\n",
    "Pydantic AI Agent (processes with context)\n",
    "     ‚Üì\n",
    "search_datasets tool (queries MCP server)\n",
    "     ‚Üì\n",
    "Response (with dataset metadata)\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **LLM (Large Language Model)**: The AI brain that processes natural language and generates responses\n",
    "   - OpenAI GPT-4o-mini: Fast and cost-effective model from OpenAI\n",
    "   - NRP Qwen3: Open-source model hosted on NRP infrastructure\n",
    "2. **Agent**: The core orchestrator that understands questions and decides when to use tools\n",
    "3. **Tool (search_datasets)**: Connects to the MCP server to search for datasets\n",
    "4. **Context**: Stores the current dataset being discussed for follow-up questions\n",
    "5. **History**: Maintains conversation flow to enable contextual responses\n",
    "6. **Chat Interface**: User-friendly UI for interaction\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To extend this agent, you could:\n",
    "- Add more tools (statistics, visualization, etc.)\n",
    "- Improve the system prompt for better responses\n",
    "- Add filtering options for search results\n",
    "- Add support for multiple datasets simultaneously"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
